{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rehamgad-1994925/Deploy-High-Availability-Web-App-using-CloudFormation/blob/main/voda_vector_search_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Download list of libraries and install them**"
      ],
      "metadata": {
        "id": "D5DW5UnMqwjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AmTNhkaf8wY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03dce58-3370-4c67-b681-6ec994b64c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vs_rag_wkshp'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 29 (delta 11), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (29/29), 455.15 KiB | 5.35 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/anantsrivast/vs_rag_wkshp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -r \"/content/vs_rag_wkshp/requirements.txt\" --upgrade --no-cache-dir"
      ],
      "metadata": {
        "id": "4GNx4lfZgCyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039b078c-f8a0-4b4c-846f-bdca32cd6a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo==4.13.0 (from -r /content/vs_rag_wkshp/requirements.txt (line 10))\n",
            "  Downloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting datasets==3.6.0 (from -r /content/vs_rag_wkshp/requirements.txt (line 11))\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: langchain==0.3.25 in /usr/local/lib/python3.11/dist-packages (from -r /content/vs_rag_wkshp/requirements.txt (line 12)) (0.3.25)\n",
            "Requirement already satisfied: sentence_transformers==4.1.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/vs_rag_wkshp/requirements.txt (line 13)) (4.1.0)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/vs_rag_wkshp/requirements.txt (line 14)) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub==0.31.4 in /usr/local/lib/python3.11/dist-packages (from -r /content/vs_rag_wkshp/requirements.txt (line 15)) (0.31.4)\n",
            "Collecting llama-cpp-python==0.3.9 (from -r /content/vs_rag_wkshp/requirements.txt (line 16))\n",
            "  Downloading llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo==4.13.0->-r /content/vs_rag_wkshp/requirements.txt (line 10))\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11))\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (0.3.60)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (2.0.41)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (4.52.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (4.13.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.9->-r /content/vs_rag_wkshp/requirements.txt (line 16))\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.9->-r /content/vs_rag_wkshp/requirements.txt (line 16)) (3.1.6)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (3.11.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.9->-r /content/vs_rag_wkshp/requirements.txt (line 16)) (3.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (3.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers==4.1.0->-r /content/vs_rag_wkshp/requirements.txt (line 13)) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (1.20.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0->-r /content/vs_rag_wkshp/requirements.txt (line 11)) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.25->-r /content/vs_rag_wkshp/requirements.txt (line 12)) (1.3.1)\n",
            "Downloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m247.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m248.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m207.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m255.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m230.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m221.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m178.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m189.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m249.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m299.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m261.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m264.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m292.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m294.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.9-cp311-cp311-linux_x86_64.whl size=4127096 sha256=50de7bbcf31fd110350ac3d166cd0ec53043c58aa7887abcf09badc595bd752d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o32_y97n/wheels/9e/8f/bf/148c8eb7d69021eccd6eae6444f3accd48347587054ffd24e5\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dnspython, diskcache, pymongo, nvidia-cusparse-cu12, nvidia-cudnn-cu12, llama-cpp-python, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 diskcache-5.6.3 dnspython-2.7.0 fsspec-2025.3.0 llama-cpp-python-0.3.9 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pymongo-4.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: MongoDB Connection**"
      ],
      "metadata": {
        "id": "tQPbNmVBrHEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pymongo import MongoClient\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "SXYO4DXPuHLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retain the quotes (\"\") when pasting the URI\n",
        "\n",
        "MONGODB_URI = \"mongodb+srv://anantsriv:e7a5x9jUMSh5bYg9@cluster0.kzo3h.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "# Initialize a MongoDB Python client\n",
        "mongodb_client = MongoClient(MONGODB_URI, appname=\"devrel.workshop.rag\")\n",
        "# Check the connection to the server\n",
        "mongodb_client.admin.command(\"ping\")"
      ],
      "metadata": {
        "id": "uDgcfG63vb2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30aed1b-bc6d-42a3-a735-51d72d79c99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ok': 1.0,\n",
              " '$clusterTime': {'clusterTime': Timestamp(1748428473, 1),\n",
              "  'signature': {'hash': b'\\xe3\\x08S\\x12\\xa2\\x0f\\x99\\xee\\x8a\\xecyVa\\x8c])v)\\xdd4',\n",
              "   'keyId': 7470271604736393222}},\n",
              " 'operationTime': Timestamp(1748428473, 1)}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Download dataset**"
      ],
      "metadata": {
        "id": "kp7U8f0jrTVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You may see a warning upon running this cell. You can ignore it.\n",
        "import pandas as pd\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "aAIh8woFvqYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the `mongodb-docs` dataset from Hugging Face\n",
        "data = load_dataset(\"mongodb/mongodb-docs\", split=\"train\")\n",
        "# Convert the dataset into a dataframe first, then into a list of Python objects/dictionaries\n",
        "docs = pd.DataFrame(data).to_dict(\"records\")"
      ],
      "metadata": {
        "id": "gYk3V1JRxwnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the number of documents in the dataset\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "ibRCfO7VzrKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview a document to understand its structure\n",
        "docs[0]"
      ],
      "metadata": {
        "id": "qsf9ulsUztKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Chunk up the data**"
      ],
      "metadata": {
        "id": "-e8pPTn0rcOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from typing import Dict, List"
      ],
      "metadata": {
        "id": "fFjrYqQ1zu3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common list of separators for text data\n",
        "separators = [\"\\n\\n\", \"\\n\", \" \", \"\", \"#\", \"##\", \"###\"]"
      ],
      "metadata": {
        "id": "HNjObhhZz4DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the `RecursiveCharacterTextSplitter` from LangChain to first split a piece of text on the list of `separators` above.\n",
        "# Then recursively merge them into tokens until the specified chunk size is reached.\n",
        "# For text data, you typically want to keep 1-2 paragraphs (~200 tokens) in a single chunk.\n",
        "# Chunk overlap of 15-20% of the chunk size is recommended to maintain context between chunks.\n",
        "# The `model_name` parameter indicates which encoder to use for tokenization, in this case GPT-4's encoder.\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    model_name=\"gpt-4\", separators=separators, chunk_size=200, chunk_overlap=30\n",
        ")"
      ],
      "metadata": {
        "id": "MEqyPmIRz6Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chunks(doc: Dict, text_field: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Chunk up a document.\n",
        "\n",
        "    Args:\n",
        "        doc (Dict): Parent document to generate chunks from.\n",
        "        text_field (str): Text field to chunk.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: List of chunked documents.\n",
        "    \"\"\"\n",
        "    # Extract the field to chunk from `doc`\n",
        "    text = doc[text_field]\n",
        "    # Split `text` using the appropriate method of the `RecursiveCharacterTextSplitter` class\n",
        "    # NOTE: `text` is a string\n",
        "    chunks = <CODE_BLOCK_1>\n",
        "\n",
        "    # Iterate through `chunks` and for each chunk:\n",
        "    # 1. Create a shallow copy of `doc`, call it `temp`\n",
        "    # 2. Set the `text_field` field in `temp` to the content of the chunk\n",
        "    # 3. Append `temp` to `chunked_data`\n",
        "    chunked_data = []\n",
        "    for chunk in chunks:\n",
        "        temp = doc.copy()\n",
        "        temp[text_field]=chunk\n",
        "        chunked_data.append(temp)\n",
        "\n",
        "    return chunked_data"
      ],
      "metadata": {
        "id": "FKG2hemMz9Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_docs = []\n",
        "# Iterate through `docs`, use the `get_chunks` function to chunk up the \"body\" field in the documents, and add the list of chunked documents to `split_docs` initialized above.\n",
        "for doc in docs:\n",
        "    chunks = <CODE_BLOCK_2>\n",
        "    split_docs.extend(chunks)"
      ],
      "metadata": {
        "id": "wHj5jVtF0CiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice that the length of `split_docs` is greater than the length of `docs` from Step 2 above\n",
        "# This is because each document in `docs` has been split into multiple chunks\n",
        "len(split_docs)"
      ],
      "metadata": {
        "id": "lGm5LMdQ0MCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview a chunked document to understand its structure\n",
        "# Note that the structure looks similar to the original docs, except the `body` field now contains smaller chunks of text\n",
        "split_docs[0]"
      ],
      "metadata": {
        "id": "BfJBZ0Ys0N_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Generate Embeddings**"
      ],
      "metadata": {
        "id": "1dDWBu72riHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the `gte-small` model using the Sentence Transformers library\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "embedding_model = SentenceTransformer(\"thenlper/gte-small\")"
      ],
      "metadata": {
        "id": "jrvnuOwZ0SK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that takes a piece of text (`text`) as input, embeds it using the `embedding_model` instantiated above and returns the embedding as a list\n",
        "# An array can be converted to a list using the `tolist()` method\n",
        "def get_embedding(text: str) -> List[float]:\n",
        "    \"\"\"\n",
        "    Generate the embedding for a piece of text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to embed.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: Embedding of the text as a list.\n",
        "    \"\"\"\n",
        "    embedding = <CODE_BLOCK_3>\n",
        "    return embedding.tolist()"
      ],
      "metadata": {
        "id": "DbyPsnyc0_uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_docs = []\n",
        "# Add an `embedding` field to each dictionary in `split_docs`\n",
        "# The `embedding` field should correspond to the embedding of the value of the `body` field\n",
        "# Use the `get_embedding` function defined above to generate the embedding\n",
        "# Append the updated dictionaries to `embedded_docs` initialized above.\n",
        "for doc in tqdm(split_docs):\n",
        "    <CODE_BLOCK_4>\n",
        "    embedded_docs.append(doc)"
      ],
      "metadata": {
        "id": "2Tu9nFwp1OKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that the length of `embedded_docs` is the same as that of `split_docs`\n",
        "len(embedded_docs)"
      ],
      "metadata": {
        "id": "5zQN1bT21Q_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Ingest data into MongoDB, create a unique db name as the first step and load data**"
      ],
      "metadata": {
        "id": "-YytPorIrtgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_unique_db_name(prefix=\"mongodb_genai_devday_rag\"):\n",
        "    \"\"\"Generate a unique database name with timestamp and UUID\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    unique_id = str(uuid.uuid4())[:8]  # Use first 8 characters of UUID\n",
        "    return f\"{prefix}_{timestamp}_{unique_id}\"\n"
      ],
      "metadata": {
        "id": "ZMI2IUzp1V5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Name of the database -- Change if needed or leave as is\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "DB_NAME = generate_unique_db_name()\n",
        "# Name of the collection -- Change if needed or leave as is\n",
        "COLLECTION_NAME = \"knowledge_base\"\n",
        "# Name of the vector search index -- Change if needed or leave as is\n",
        "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\""
      ],
      "metadata": {
        "id": "-rfSqJxB1UEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = mongodb_client[DB_NAME]"
      ],
      "metadata": {
        "id": "7HvFfHAg5f-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the `COLLECTION_NAME` collection.\n",
        "# Use the `db` and collection name defined above.\n",
        "collection = db[COLLECTION_NAME]"
      ],
      "metadata": {
        "id": "2HNFMwcH637k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bulk delete all existing records from the collection defined above\n",
        "collection.delete_many({})"
      ],
      "metadata": {
        "id": "3CIXnPwc67cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bulk insert `embedded_docs` into the collection defined above -- should be a one-liner\n",
        "<CODE_BLOCK_5>\n",
        "\n",
        "print(f\"Ingested {collection.count_documents({})} documents into the {COLLECTION_NAME} collection.\")"
      ],
      "metadata": {
        "id": "ICEUwD4D69zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 7: Perform vector search on your data\n",
        "\n",
        "### Define a vector search function\n",
        "\n",
        "沒 https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to the \"Basic Example\")"
      ],
      "metadata": {
        "id": "3_bS43gar5Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vector index definition specifying:\n",
        "# path: Path to the embeddings field\n",
        "# numDimensions: Number of embedding dimensions- depends on the embedding model used\n",
        "# similarity: Similarity metric. One of cosine, euclidean, dotProduct.\n",
        "model = {\n",
        "    \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
        "    \"type\": \"vectorSearch\",\n",
        "    \"definition\": {\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"type\": \"vector\",\n",
        "                \"path\": \"embedding\",\n",
        "                \"numDimensions\": 384,\n",
        "                \"similarity\": \"cosine\",\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "uB8Bvj4w6_-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vector search index with the above definition for the `collection` collection\n",
        "collection.create_search_index(model)"
      ],
      "metadata": {
        "id": "WX2QxNAoK5ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to retrieve relevant documents for a user query using vector search\n",
        "def vector_search(user_query: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Retrieve relevant documents for a user query using vector search.\n",
        "\n",
        "    Args:\n",
        "    user_query (str): The user's query string.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of matching documents.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate embedding for the `user_query` using the `get_embedding` function defined in Step 4\n",
        "    query_embedding = <CODE_BLOCK_6>\n",
        "\n",
        "    # Define an aggregation pipeline consisting of a $vectorSearch stage, followed by a $project stage\n",
        "    # Set the number of candidates to 150 and only return the top 5 documents from the vector search\n",
        "    # In the $project stage, exclude the `_id` field and include only the `body` field and `vectorSearchScore`\n",
        "    # NOTE: Use variables defined previously for the `index`, `queryVector` and `path` fields in the $vectorSearch stage\n",
        "    pipeline = <CODE_BLOCK_7>\n",
        "\n",
        "    # Execute the aggregation `pipeline` and store the results in `results`\n",
        "    results = <CODE_BLOCK_8>\n",
        "    return list(results)"
      ],
      "metadata": {
        "id": "js8B44hYInng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_search(\"What are some best practices for data backups in MongoDB?\")"
      ],
      "metadata": {
        "id": "QEgk55RxLNV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 8: Build the RAG application**"
      ],
      "metadata": {
        "id": "GqVteVOYswby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O phi-2.gguf https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "9YnsU8hWWoBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(model_path=\"phi-2.gguf\", n_ctx=1024)"
      ],
      "metadata": {
        "id": "6IUSGJ1TrT8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to create the user prompt for our RAG application\n",
        "def create_prompt(user_query: str) -> str:\n",
        "    \"\"\"\n",
        "    Create a chat prompt that includes the user query and retrieved context.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's query string.\n",
        "\n",
        "    Returns:\n",
        "        str: The chat prompt string.\n",
        "    \"\"\"\n",
        "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function defined in Step 7\n",
        "    context = <CODE_BLOCK_9>\n",
        "    # Join the retrieved documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
        "    context = \"\\n\\n\".join([doc.get('body') for doc in context])\n",
        "    # Prompt consisting of the question and relevant context to answer it\n",
        "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "YBIa-jbCsu2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chat_prompt(messages):\n",
        "    \"\"\"Formats a list of role/content messages into a prompt string\"\"\"\n",
        "    prompt = \"\"\n",
        "    for msg in messages:\n",
        "        if msg[\"role\"] == \"system\":\n",
        "            prompt += f\"<|system|>\\n{msg['content']}\\n\"\n",
        "        elif msg[\"role\"] == \"user\":\n",
        "            prompt += f\"<|user|>\\n{msg['content']}\\n\"\n",
        "        elif msg[\"role\"] == \"assistant\":\n",
        "            prompt += f\"<|assistant|>\\n{msg['content']}\\n\"\n",
        "    prompt += \"<|assistant|>\\n\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "RZ-MIpaLXKkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to answer user queries\n",
        "def generate_answer(user_query: str) -> None:\n",
        "    \"\"\"\n",
        "    Generate an answer to the user query.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's query string.\n",
        "    \"\"\"\n",
        "    # Use the `create_prompt` function above to create a chat prompt\n",
        "    prompt = <CODE_BLOCK_10>\n",
        "    # Format the message to the LLM in the format {\"role\": <role_value>, \"content\": <content_value>}\n",
        "    # The role value for user messages must be \"user\"\n",
        "    # Use the `prompt` created above to populate the `content` field in the chat message\n",
        "    messages = <CODE_BLOCK_11>\n",
        "    # Send the chat messages to LLM\n",
        "    prompt=format_chat_prompt(messages)\n",
        "    #print(prompt)\n",
        "    response = llm(prompt, max_tokens=500)\n",
        "    # Print the response\n",
        "    #response = requests.post(url, json=messages)\n",
        "    print(response[\"choices\"][0][\"text\"])\n",
        "    #print(response.json()['response'])\n",
        "    # print(response.json()[\"text\"])"
      ],
      "metadata": {
        "id": "u5r8l2CQsxvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_answer(\"What are some best practices for data backups in MongoDB?\")"
      ],
      "metadata": {
        "id": "R1IMYkiNs2Nf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}